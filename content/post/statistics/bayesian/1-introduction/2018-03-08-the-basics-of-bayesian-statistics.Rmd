---
title: Introduction
author: Alireza Behzadnia
date: '2018-03-08'
categories:
  - Bayesian
  - R
slug: bayesian-introduction
Weight: 1
---

Bayes Rule in the discrete form, which allows us to calculate the
probability of an event A given event B. This is what is called
a conditional probability, and we will be working with conditional
probabilities a whole lot in this module and also in the remainder of this course. One example of a conditional
probability is the false positive or false negative rate of a medical test. 

$$P(A|B)= \frac{P(A\bigcap B)}{P(B)}$$

Thomas Bayes,
who lived between 1702 and 1761, was a mathematician who established
a mathematical basis for probability inference, that is, a means
of calculating from the number of times an event has not occurred the probability
that it will occur in future trials. He wrote his findings on probability
in essay towards solving a problem in the Doctrine of Chances
published in 1763 after his death. Thomas Bayes' contributions
are immortalized by naming a fundamental proposition in probability
called Bayes' rule after him. Lets recalculate that same
probability using Bayes' rule. 

***
## HIV testing
n example of an organization that
developed a rigorous testing for HIV was the U.S. Military, which used the
following procedure for testing recruits. First, all applicants were screened with
an enzyme linked immune absorbent SA, which is commonly referred to as an ELISA. If the samples tested
positive then two more rounds of the same ELISA were performed. If either of those test yielded a positive
result, then two Western Blot assays, that were more cumbersome to conduct,
but had higher accuracy were performed. Only if both of those tests were positive,
did the military determine the recruit to have an HIV infection,
based on papers published at the time. For the ELISA, the true positive also
refer to as the sensitivity of the test was around 93%, and
the true negative rate also refer to as the specificity of
the test was around 99%. For the Western block,
the sensitivity was around 99.9% and the specificity was around 99.1%. We also know that by the mid 1980's, it was estimated that 1.48 / 1000
adult Americans were HIV positive.

n this lesson, we will use Bayes Rule to
calculate the probability that a recruit who tested positive in the first
ELISA actually has HIV. Then in the next lesson, we will
consider the sequential testing results.

Sensitivity (true positive) = $P(+ve |HIV+) = 0.93$ and specifity (true negative) $P(-ve|HIV-) = 0.99$. 

## Prior probability
ior to any testing, what probability
should be assigned for recruit having HIV? Given that we don't have any additional
information about this recruit, our best guess is that they are a randomly
sampled individual from this population. Hence, the prior probability we
assign to this recruit having HIV is simply the prevalence of
the disease in the population. That is, probability of HIV is 000148. This is called the prior probability.


he prior probability of the hypothesis
that the recruit has HIV is 0.00148, and the prior for
the competing hypothesis that the recruit does not have HIV is
simply the compliment of this probability.
### What was the probability of having HIV given the first ELISA test was positive for an American patient in mid 1980's?

The questions is asking: $P(HIV|+ve) =\frac{P(HIV\bigcap +ve)}{P(+ve)}$

* *Prior probability*  
    * $\small P(HIV+)=\frac{1.48}{1000}=0.00148$

* *Probability of being positive and testing positive*:
    * ELISA's sensitivity is $\small P(+ve |HIV+) = 0.93$
    * $\small P(+ve \cap HIV+): 0.93 \times 0.00148=0.00138$  


* *Probability of testing positive*
  * Patient could have been HIV+ve or HIV-ve
  - ELISA's specificity $P(-ve|HIV-)=0.99~ P(+ve|HIV-)=0.01$; therefore $0.99825*0.01=0.0099852$ 
  - $P(+ve) = 0.0013764+0.009852 = 0.0112284$
* **conclusion**
  - $P(HIV|+ve) = \frac{0.0013764}{0.0112284} ~ 0.12$
  
### What was the probability of having HIV given the first and second ELISA tests were positive for an American patient in mid 1980's?

* Prior probability of having HIV for this patient should be updated with the posteriori from the previous tests.
* Prior probability of having HIV $P(HIV+) = 0.12$
* *Probability of being positive and testing positive*:
  - ELISA's sensitivity is $P(+ve |HIV+) = 0.93$ ;therefore $0.93*0.12=0.1116$
  - $P(+ve \bigcap HIV+) = 0.11$  
* *Probability of testing positive*
  - Patient could have been HIV+ve or HIV-ve
  - ELISA's specificity $P(-ve|HIV-)=0.99~ P(+ve|HIV-)=0.01$; therefore $(1-0.12)*0.01=0.0092$ 
  - $P(+ve) = 0.01+0.11 = 0.12$
* **Probability of a patient having HIV and testing positive twice is**
  - $\frac{0.11}{0.12} ~ 0.92$


## Bayesian 
One definition of probability
of an event is its relative frequency in
a large number of trials. For example, if you can repeat
flipping a coin indefinitely and count how many heads you get and divide that number by the number of flips,
the value you obtain should be 0.5. In other words the probability of event
E is defined as the proportion of times the event occurs and
n trials when n goes to infinity. This is the frequentist definition
of probability, suppose now that you're indifferent between winning
a dollar if event E occurs or winning a dollar if you draw a blue chip
from a box with 1,000 x p blue chips and 1,000 x (1-p) white chips. This means that you're equating
the probability of events E, that's P(E), to the probability of drawing
a blue chip from this box. In other words P(E) = p. This definition of probability,
based on your degree of belief, is the Bayesian definition. 


The frequentist definition of probability
allows to define a probability for the confidence interval procedure but
not for specific fixed sample. And the case of a specific fixed sample,
when the data do not change, we will either always capture the true
parameter or never capture it. In other words, for given confidence interval the true
parameter is either in it or not. This is the same as saying that
the probability that a given confidence interval captures the true parameter,
is either zero or one.


he Bayesian definition
is a bit more flexible. Since it's a measure of belief it allows
us to describe the unknown true parameter not as a fixed value but
with a probability distribution. This will let us construct something
like a confidence interval, except we will be able to
make probabilistic statements about the parameter
falling within that range.